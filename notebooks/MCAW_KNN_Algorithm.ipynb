{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCAW-KNN: Multi-Class Adaptive Weighted K-Nearest Neighbors\n",
    "\n",
    "This notebook implements the MCAW-KNN algorithm as described in the accompanying paper. The algorithm combines:\n",
    "\n",
    "1. **Smooth Local Region Building** - Uses Bayesian smoothing, Wilson score intervals, and Gaussian kernels to construct representative local regions\n",
    "2. **Global Weight Matrix Computation** - Calculates class-specific feature weights using methods like LDA, F-score, or centroid-based approaches\n",
    "3. **Binary Class Weight Correction** - Corrects weight vectors using generalized Rayleigh quotient manifold constraints\n",
    "4. **Weighted KNN Classification** - Performs distance-weighted voting for final classification\n",
    "\n",
    "**Author:** Meimei.Huang  \n",
    "**Date:** January 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.linalg import logm, fractional_matrix_power, eigh\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from scipy import stats\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Smooth Local Region Builder\n",
    "\n",
    "This class constructs local regions around a target point using:\n",
    "- **Bayesian Smoothing**: Handles small sample sizes with prior information\n",
    "- **Wilson Score Interval**: Provides conservative confidence estimates\n",
    "- **Gaussian Kernel**: Adjusts distances based on class representativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothLocalRegionBuilder:\n",
    "    \"\"\"\n",
    "    Local region builder based on Bayesian smoothing, Wilson smoothing, and Gaussian kernel.\n",
    "    Provides smooth class representativeness evaluation and distance adjustment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k_region : int\n",
    "        Candidate set size for each class\n",
    "    region_size : int\n",
    "        Target local region size\n",
    "    alpha : float\n",
    "        Bayesian prior parameter α\n",
    "    beta : float\n",
    "        Bayesian prior parameter β\n",
    "    confidence_level : float\n",
    "        Wilson interval confidence level\n",
    "    sigma : float\n",
    "        Gaussian kernel bandwidth parameter\n",
    "    prior_strength : int\n",
    "        Prior strength for Bayesian estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k_region=15, region_size=15, alpha=1.0, beta=1.0, \n",
    "                 confidence_level=0.95, sigma=1.0, prior_strength=5):\n",
    "        self.k_region = k_region\n",
    "        self.region_size = region_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.confidence_level = confidence_level\n",
    "        self.sigma = sigma\n",
    "        self.prior_strength = prior_strength\n",
    "        \n",
    "        # Storage for intermediate computation results\n",
    "        self.sample_results = {}  # Detailed results for each sample\n",
    "        self.class_tightness = {}  # Class tightness values\n",
    "        self.class_candidates = {}  # Candidate sets per class\n",
    "    \n",
    "    def fit(self, X, y, row_ids):\n",
    "        \"\"\"Fit the region builder with training data.\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.row_ids = row_ids\n",
    "        return self\n",
    "    \n",
    "    def compute_class_tightness(self, class_label, indices):\n",
    "        \"\"\"Compute class tightness (inverse of average distance to centroid).\"\"\"\n",
    "        class_indices = indices[self.y[indices] == class_label]\n",
    "        if len(class_indices) == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        class_samples = self.X[class_indices]\n",
    "        if len(class_samples) <= 1:\n",
    "            return 1.0\n",
    "        \n",
    "        centroid = np.mean(class_samples, axis=0)\n",
    "        avg_distance = np.mean(np.linalg.norm(class_samples - centroid, axis=1))\n",
    "        tightness = 1.0 / (avg_distance + 1e-8)\n",
    "        return tightness\n",
    "    \n",
    "    def compute_backward_rank(self, target_point, sample_point, candidate_points):\n",
    "        \"\"\"Compute backward rank (rank of target from sample's perspective).\"\"\"\n",
    "        if len(candidate_points) == 0:\n",
    "            return 1\n",
    "        \n",
    "        distances_to_sample = [np.linalg.norm(candidate - sample_point) \n",
    "                              for candidate in candidate_points]\n",
    "        target_to_sample_dist = np.linalg.norm(target_point - sample_point)\n",
    "        all_distances = distances_to_sample + [target_to_sample_dist]\n",
    "        sorted_indices = np.argsort(all_distances)\n",
    "        rank = np.where(sorted_indices == len(all_distances) - 1)[0][0] + 1\n",
    "        return rank\n",
    "    \n",
    "    def bayesian_smoothing(self, k, n, alpha=None, beta=None):\n",
    "        \"\"\"\n",
    "        Compute Bayesian smoothed probability p_bayesian.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int\n",
    "            Number of successes (e.g., count of high-ranked samples)\n",
    "        n : int\n",
    "            Total number of trials\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Bayesian smoothed probability\n",
    "        \"\"\"\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "        if beta is None:\n",
    "            beta = self.beta\n",
    "        \n",
    "        if n == 0:\n",
    "            return 0.5  # Default prior\n",
    "        \n",
    "        p_bayesian = (k + alpha) / (n + alpha + beta)\n",
    "        return p_bayesian\n",
    "    \n",
    "    def wilson_score_interval(self, p, n, z_score=None):\n",
    "        \"\"\"\n",
    "        Compute Wilson score interval lower bound p_wilson.\n",
    "        \n",
    "        This provides a conservative estimate of the true proportion.\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "            return p\n",
    "        \n",
    "        if z_score is None:\n",
    "            z_score = stats.norm.ppf(1 - (1 - self.confidence_level) / 2)\n",
    "        \n",
    "        denominator = 1 + z_score**2 / n\n",
    "        centre = (p + z_score**2 / (2 * n)) / denominator\n",
    "        width = (z_score * np.sqrt((p * (1 - p)) / n + z_score**2 / (4 * n**2))) / denominator\n",
    "        \n",
    "        # Use confidence lower bound as conservative estimate\n",
    "        p_wilson = centre - width\n",
    "        return max(0, min(1, p_wilson))\n",
    "    \n",
    "    def gaussian_kernel_penalty(self, penalty, sigma=None):\n",
    "        \"\"\"Compute Gaussian kernel adjustment factor.\"\"\"\n",
    "        if sigma is None:\n",
    "            sigma = self.sigma\n",
    "        adjustment_factor = np.exp(-0.5 * (penalty / sigma) ** 2)\n",
    "        return adjustment_factor\n",
    "    \n",
    "    def calculate_penalty(self, p_wilson, method='linear'):\n",
    "        \"\"\"Calculate penalty term based on p_wilson.\"\"\"\n",
    "        if method == 'linear':\n",
    "            penalty = 1.0 - p_wilson  # Linear penalty\n",
    "        elif method == 'quadratic':\n",
    "            penalty = (1.0 - p_wilson) ** 2  # Quadratic penalty\n",
    "        elif method == 'sigmoid':\n",
    "            penalty = 1.0 / (1.0 + np.exp(-10 * (p_wilson - 0.5)))  # Sigmoid penalty\n",
    "        else:\n",
    "            penalty = 1.0 - p_wilson\n",
    "        return penalty\n",
    "    \n",
    "    def compute_representativeness(self, R, r, N, sample_id):\n",
    "        \"\"\"\n",
    "        Compute sample's class representativeness combining Bayesian smoothing,\n",
    "        Wilson smoothing, and Gaussian kernel.\n",
    "        \n",
    "        Returns adjustment factor and all intermediate results.\n",
    "        \"\"\"\n",
    "        # Step 1: Initial representativeness assessment (based on backward rank)\n",
    "        initial_representativeness = r  # Normalized backward rank\n",
    "        \n",
    "        # Step 2: Bayesian smoothing (handles small sample cases)\n",
    "        k_bayesian = max(1, int(r * N))  # Success count: high-ranked samples\n",
    "        n_bayesian = N  # Total trials\n",
    "        p_bayesian = self.bayesian_smoothing(k_bayesian, n_bayesian)\n",
    "        \n",
    "        # Step 3: Wilson smoothing (provides conservative confidence interval estimate)\n",
    "        p_wilson = self.wilson_score_interval(initial_representativeness, N)\n",
    "        \n",
    "        # Step 4: Tightness-weighted adjustment\n",
    "        tightness_weight = np.tanh(R - 1)  # Positive adjustment when R>1, negative when R<1\n",
    "        weighted_p_wilson = p_wilson * (1 + 0.3 * tightness_weight)\n",
    "        weighted_p_wilson = max(0, min(1, weighted_p_wilson))\n",
    "        \n",
    "        # Step 5: Calculate penalty term\n",
    "        penalty = self.calculate_penalty(weighted_p_wilson, method='sigmoid')\n",
    "        \n",
    "        # Step 6: Gaussian kernel adjustment factor\n",
    "        adjustment_factor = self.gaussian_kernel_penalty(penalty)\n",
    "        \n",
    "        # Store all intermediate results\n",
    "        results = {\n",
    "            'sample_id': sample_id,\n",
    "            'R': R,\n",
    "            'r': r,\n",
    "            'N': N,\n",
    "            'initial_representativeness': initial_representativeness,\n",
    "            'p_bayesian': p_bayesian,\n",
    "            'p_wilson': p_wilson,\n",
    "            'weighted_p_wilson': weighted_p_wilson,\n",
    "            'penalty': penalty,\n",
    "            'adjustment_factor': adjustment_factor,\n",
    "            'tightness_weight': tightness_weight\n",
    "        }\n",
    "        return adjustment_factor, results\n",
    "    \n",
    "    def build_local_region(self, target_point, verbose=False):\n",
    "        \"\"\"\n",
    "        Build local region based on smooth representativeness evaluation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        target_point : array-like\n",
    "            The query point for which to build the local region\n",
    "        verbose : bool\n",
    "            Whether to print detailed progress information\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            Indices of samples in the local region\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"=== Building Smooth Local Region ===\")\n",
    "        \n",
    "        # Reset result storage\n",
    "        self.sample_results = {}\n",
    "        self.class_tightness = {}\n",
    "        self.class_candidates = {}\n",
    "        \n",
    "        # Get all classes\n",
    "        unique_classes = np.unique(self.y)\n",
    "        if verbose:\n",
    "            print(f\"Classes in dataset: {unique_classes}\")\n",
    "        \n",
    "        # Step 1: Build candidate set for each class and compute tightness\n",
    "        candidate_indices = []\n",
    "        for cls in unique_classes:\n",
    "            # Get all samples of this class\n",
    "            class_indices = np.where(self.y == cls)[0]\n",
    "            if len(class_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Compute distances to target point\n",
    "            distances = [np.linalg.norm(self.X[i] - target_point) for i in class_indices]\n",
    "            \n",
    "            # Select k_region nearest samples\n",
    "            sorted_indices = np.argsort(distances)[:self.k_region]\n",
    "            cls_candidates = [class_indices[i] for i in sorted_indices]\n",
    "            \n",
    "            # Compute class tightness\n",
    "            tightness = self.compute_class_tightness(cls, class_indices)\n",
    "            \n",
    "            self.class_candidates[cls] = cls_candidates\n",
    "            self.class_tightness[cls] = tightness\n",
    "            candidate_indices.extend(cls_candidates)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Class {cls}: candidate size {len(cls_candidates)}, tightness: {tightness:.4f}\")\n",
    "        \n",
    "        # Step 2: Compute adjusted distances for each candidate sample\n",
    "        if verbose:\n",
    "            print(\"\\n=== Computing Sample Adjusted Distances ===\")\n",
    "        \n",
    "        all_candidates = list(set(candidate_indices))\n",
    "        adjusted_distances = []\n",
    "        \n",
    "        for candidate_idx in all_candidates:\n",
    "            cls = self.y[candidate_idx]\n",
    "            row_id = self.row_ids[candidate_idx]\n",
    "            \n",
    "            if cls not in self.class_candidates:\n",
    "                continue\n",
    "            \n",
    "            # Get class context\n",
    "            cls_candidates = self.class_candidates[cls]\n",
    "            candidate_points = self.X[cls_candidates]\n",
    "            sample_point = self.X[candidate_idx]\n",
    "            \n",
    "            # Compute original distance\n",
    "            original_distance = np.linalg.norm(sample_point - target_point)\n",
    "            \n",
    "            # Compute backward rank\n",
    "            backward_rank = self.compute_backward_rank(target_point, sample_point, candidate_points)\n",
    "            \n",
    "            # Compute normalized backward rank r\n",
    "            N = len(cls_candidates)\n",
    "            if N > 1:\n",
    "                r = 1 - (backward_rank - 1) / (N - 1)\n",
    "            else:\n",
    "                r = 1.0\n",
    "            \n",
    "            # Get relative tightness R (current class tightness / average tightness)\n",
    "            avg_tightness = np.mean(list(self.class_tightness.values()))\n",
    "            R = self.class_tightness[cls] / (avg_tightness + 1e-8)\n",
    "            \n",
    "            # Compute comprehensive representativeness adjustment factor\n",
    "            adjustment_factor, results = self.compute_representativeness(R, r, N, row_id)\n",
    "            \n",
    "            # Compute adjusted distance\n",
    "            adjusted_distance = original_distance * adjustment_factor\n",
    "            \n",
    "            # Store complete results\n",
    "            results.update({\n",
    "                'row_id': row_id,\n",
    "                'class': cls,\n",
    "                'original_distance': original_distance,\n",
    "                'adjusted_distance': adjusted_distance,\n",
    "                'backward_rank': backward_rank,\n",
    "                'R': R,\n",
    "                'r': r\n",
    "            })\n",
    "            self.sample_results[row_id] = results\n",
    "            adjusted_distances.append((candidate_idx, adjusted_distance, cls, row_id))\n",
    "        \n",
    "        # Step 3: Select samples by class proportion to build local region\n",
    "        if verbose:\n",
    "            print(\"\\n=== Building Local Region by Class Proportion ===\")\n",
    "        \n",
    "        local_region = []\n",
    "        class_distribution = Counter(self.y)\n",
    "        total_samples = sum(class_distribution.values())\n",
    "        \n",
    "        # Calculate target sample count for each class\n",
    "        class_targets = {}\n",
    "        for cls, count in class_distribution.items():\n",
    "            target_count = max(1, int(self.region_size * count / total_samples))\n",
    "            class_targets[cls] = min(target_count, len(self.class_candidates.get(cls, [])))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Target class distribution: {class_targets}\")\n",
    "        \n",
    "        # Select samples with smallest adjusted distances for each class\n",
    "        for cls, target_count in class_targets.items():\n",
    "            if cls not in self.class_candidates or target_count == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get candidate samples of this class\n",
    "            cls_candidates = [idx for idx, _, cls_val, row_id in adjusted_distances if cls_val == cls]\n",
    "            if not cls_candidates:\n",
    "                continue\n",
    "            \n",
    "            # Sort by adjusted distance\n",
    "            cls_candidates_sorted = sorted(\n",
    "                cls_candidates,\n",
    "                key=lambda idx: next(adj_dist for i, adj_dist, c, rid in adjusted_distances \n",
    "                                   if i == idx and c == cls)\n",
    "            )[:target_count]\n",
    "            local_region.extend(cls_candidates_sorted)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Class {cls}: selected {len(cls_candidates_sorted)} samples\")\n",
    "        \n",
    "        # Step 4: If region size is insufficient, add samples with smallest adjusted distances\n",
    "        if len(local_region) < self.region_size:\n",
    "            remaining = self.region_size - len(local_region)\n",
    "            used_set = set(local_region)\n",
    "            available_candidates = [idx for idx, _, _, _ in adjusted_distances \n",
    "                                  if idx not in used_set]\n",
    "            if available_candidates:\n",
    "                available_sorted = sorted(\n",
    "                    available_candidates,\n",
    "                    key=lambda idx: next(adj_dist for i, adj_dist, c, rid in adjusted_distances \n",
    "                                       if i == idx)\n",
    "                )[:remaining]\n",
    "                local_region.extend(available_sorted)\n",
    "                if verbose:\n",
    "                    print(f\"Added {len(available_sorted)} supplementary samples\")\n",
    "        \n",
    "        # Final size adjustment\n",
    "        local_region = local_region[:self.region_size]\n",
    "        \n",
    "        return local_region\n",
    "    \n",
    "    def get_detailed_results(self, sort_by='p_bayesian'):\n",
    "        \"\"\"Get detailed results sorted by specified metric.\"\"\"\n",
    "        if not self.sample_results:\n",
    "            return []\n",
    "        \n",
    "        sort_keys = {\n",
    "            'p_bayesian': lambda x: x[1]['p_bayesian'],\n",
    "            'p_wilson': lambda x: x[1]['p_wilson'],\n",
    "            'penalty': lambda x: x[1]['penalty'],\n",
    "            'adjustment_factor': lambda x: x[1]['adjustment_factor'],\n",
    "            'adjusted_distance': lambda x: x[1]['adjusted_distance']\n",
    "        }\n",
    "        \n",
    "        reverse = sort_by not in ['penalty', 'adjustment_factor', 'adjusted_distance']\n",
    "        \n",
    "        if sort_by in sort_keys:\n",
    "            return sorted(self.sample_results.items(), key=sort_keys[sort_by], reverse=reverse)\n",
    "        return list(self.sample_results.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Global Weight Matrix\n",
    "\n",
    "Computes class-specific feature weights using various methods:\n",
    "- **LDA (Linear Discriminant Analysis)**: Based on generalized Rayleigh quotient\n",
    "- **F-Score**: Using ANOVA F-statistics\n",
    "- **Centroid**: Based on point-to-centroid ratios\n",
    "- **Inter-class Difference**: Using precision matrix-weighted mean differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalWeightMatrix:\n",
    "    \"\"\"\n",
    "    Computes global feature weights for each class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    method : str\n",
    "        Weight computation method: 'lda', 'f_score', 'centroid', 'inter_class_difference'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='lda'):\n",
    "        self.method = method\n",
    "        self.global_weights = None\n",
    "        self.class_weight_dict = None  # Class weight dictionary\n",
    "        self.feature_names = None\n",
    "        self.class_names = None\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None, feature_names=None, class_names=None):\n",
    "        \"\"\"\n",
    "        Fit the weight calculator on training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training feature matrix\n",
    "        y : array-like of shape (n_samples,), optional\n",
    "            Class labels\n",
    "        feature_names : list, optional\n",
    "            Names of features\n",
    "        class_names : list, optional\n",
    "            Names of classes\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        if y is not None:\n",
    "            y = np.array(y)\n",
    "        \n",
    "        self.feature_names = feature_names\n",
    "        self.class_names = class_names\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        if y is None:\n",
    "            # Unsupervised case\n",
    "            self.global_weights = self._compute_global_weights(X_scaled)\n",
    "            self.global_weights = self.global_weights.reshape(1, -1)\n",
    "            self.class_weight_dict = {'global': self.global_weights[0]}\n",
    "        else:\n",
    "            # Supervised case: compute weights for each class\n",
    "            unique_classes = np.unique(y)\n",
    "            n_classes = len(unique_classes)\n",
    "            n_features = X.shape[1]\n",
    "            \n",
    "            self.global_weights = np.zeros((n_classes, n_features))\n",
    "            self.class_weight_dict = {}\n",
    "            \n",
    "            for i, class_label in enumerate(unique_classes):\n",
    "                weight_vector = self._compute_global_weights(X_scaled, y, class_label)\n",
    "                self.global_weights[i] = weight_vector\n",
    "                self.class_weight_dict[class_label] = weight_vector\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_weight_matrix(self):\n",
    "        \"\"\"Return the weight matrix.\"\"\"\n",
    "        return self.global_weights\n",
    "    \n",
    "    def get_class_weight_dict(self):\n",
    "        \"\"\"Return the class weight dictionary.\"\"\"\n",
    "        return self.class_weight_dict\n",
    "    \n",
    "    def get_weight_by_class(self, class_identifier):\n",
    "        \"\"\"\n",
    "        Get weight vector by class identifier.\n",
    "        Supports class name or original label.\n",
    "        \"\"\"\n",
    "        if self.class_weight_dict is None:\n",
    "            raise ValueError(\"Please call fit() method first to train the model\")\n",
    "        \n",
    "        if class_identifier in self.class_weight_dict:\n",
    "            return self.class_weight_dict[class_identifier]\n",
    "        else:\n",
    "            str_identifier = str(class_identifier)\n",
    "            if str_identifier in self.class_weight_dict:\n",
    "                return self.class_weight_dict[str_identifier]\n",
    "            else:\n",
    "                print(f\"Warning: Class '{class_identifier}' weight not found, using uniform weights\")\n",
    "                n_features = self.global_weights.shape[1] if self.global_weights is not None else 1\n",
    "                return np.ones(n_features) / n_features\n",
    "    \n",
    "    def _compute_global_weights(self, X, y=None, current_class=None):\n",
    "        \"\"\"Dispatch to appropriate weight computation method.\"\"\"\n",
    "        if y is not None and current_class is not None:\n",
    "            methods = {\n",
    "                'inter_class_difference': self._inter_class_difference_weights,\n",
    "                'f_score': self._f_score_weights,\n",
    "                'centroid': self._centroid_weights,\n",
    "                'lda': self._rayleigh_quotient\n",
    "            }\n",
    "            method_func = methods.get(self.method, self._inverse_covariance_weights)\n",
    "            return method_func(X, y, current_class)\n",
    "        else:\n",
    "            return self._inverse_covariance_weights(X, y, current_class)\n",
    "    \n",
    "    def _rayleigh_quotient(self, X, y, current_class):\n",
    "        \"\"\"\n",
    "        LDA weight computation based on generalized Rayleigh quotient.\n",
    "        Binary mode: current class vs. other classes.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create binary labels: current class vs. others\n",
    "            y_binary = (y == current_class).astype(int)\n",
    "            \n",
    "            # Get samples from both classes\n",
    "            class1_mask = (y_binary == 1)  # Current class\n",
    "            class0_mask = (y_binary == 0)  # Other classes\n",
    "            X1 = X[class1_mask]  # Current class samples\n",
    "            X0 = X[class0_mask]  # Other class samples\n",
    "            \n",
    "            n1 = len(X1)\n",
    "            n0 = len(X0)\n",
    "            n_features = X.shape[1]\n",
    "            \n",
    "            # Check if sample count is sufficient\n",
    "            if n1 < 2 or n0 < 1:\n",
    "                return np.ones(n_features) / n_features\n",
    "            \n",
    "            # Compute mean vectors for both classes\n",
    "            mu1 = np.mean(X1, axis=0)  # Current class mean\n",
    "            mu0 = np.mean(X0, axis=0)  # Other class mean\n",
    "            \n",
    "            # Compute between-class scatter direction\n",
    "            mean_diff = mu1 - mu0\n",
    "            \n",
    "            # Compute within-class scatter matrix Sw\n",
    "            S1 = np.cov(X1.T) * (n1 - 1) if n1 > 1 else np.zeros((n_features, n_features))\n",
    "            S0 = np.cov(X0.T) * (n0 - 1) if n0 > 1 else np.zeros((n_features, n_features))\n",
    "            Sw = S1 + S0\n",
    "            \n",
    "            # Add regularization to prevent singular matrix\n",
    "            epsilon = 1e-6\n",
    "            Sw_reg = Sw + epsilon * np.eye(n_features)\n",
    "            \n",
    "            # Compute LDA weight vector: w = Sw^{-1} * (mu1 - mu0)\n",
    "            try:\n",
    "                Sw_inv = np.linalg.pinv(Sw_reg)\n",
    "                weights = np.dot(Sw_inv, mean_diff)\n",
    "            except np.linalg.LinAlgError:\n",
    "                weights = mean_diff\n",
    "            \n",
    "            # Handle NaN or Inf values\n",
    "            weights = np.nan_to_num(weights, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "            \n",
    "            # Take absolute value and normalize\n",
    "            weights = np.abs(weights)\n",
    "            if np.sum(weights) > 0:\n",
    "                weights = weights / np.sum(weights)\n",
    "            else:\n",
    "                weights = np.ones(n_features) / n_features\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Rayleigh quotient weight computation error: {e}\")\n",
    "            n_features = X.shape[1]\n",
    "            return np.ones(n_features) / n_features\n",
    "    \n",
    "    def _centroid_weights(self, X, y, current_class):\n",
    "        \"\"\"\n",
    "        Centroid method: weights based on ratio of each point to global centroid.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            class_mask = (y == current_class)\n",
    "            X_class = X[class_mask]\n",
    "            n_samples_class = len(X_class)\n",
    "            n_features = X.shape[1]\n",
    "            \n",
    "            if n_samples_class == 0:\n",
    "                return np.ones(n_features) / n_features\n",
    "            \n",
    "            # Compute global centroid (mean of all points)\n",
    "            centroid_all = np.mean(X, axis=0)\n",
    "            epsilon = 1e-8\n",
    "            centroid_all_safe = centroid_all + epsilon\n",
    "            \n",
    "            # For each point in current class, compute ratio to global centroid\n",
    "            ratio_vectors = []\n",
    "            for i in range(n_samples_class):\n",
    "                x_point = X_class[i]\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    ratio = np.divide(x_point, centroid_all_safe)\n",
    "                ratio = np.nan_to_num(ratio, nan=1.0, posinf=1.0, neginf=0.0)\n",
    "                ratio_abs = np.abs(ratio)\n",
    "                ratio_vectors.append(ratio_abs)\n",
    "            \n",
    "            # Compute mean ratio vector as weights\n",
    "            mean_ratio = np.mean(ratio_vectors, axis=0) if ratio_vectors else np.ones(n_features)\n",
    "            \n",
    "            # Normalize\n",
    "            if np.sum(mean_ratio) > 0:\n",
    "                weights = mean_ratio / np.sum(mean_ratio)\n",
    "            else:\n",
    "                weights = np.ones(n_features) / n_features\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Centroid weight computation error: {e}\")\n",
    "            return np.ones(X.shape[1]) / X.shape[1]\n",
    "    \n",
    "    def _inter_class_difference_weights(self, X, y, current_class):\n",
    "        \"\"\"Compute weights based on inter-class differences using precision matrix.\"\"\"\n",
    "        try:\n",
    "            current_class_mask = (y == current_class)\n",
    "            other_class_mask = (y != current_class)\n",
    "            X_current = X[current_class_mask]\n",
    "            X_other = X[other_class_mask]\n",
    "            \n",
    "            if len(X_current) < 2 or len(X_other) < 1:\n",
    "                n_features = X.shape[1]\n",
    "                return np.ones(n_features) / n_features\n",
    "            \n",
    "            cov_estimator_current = LedoitWolf().fit(X_current)\n",
    "            sigma_current = cov_estimator_current.covariance_\n",
    "            precision_current = np.linalg.pinv(sigma_current)\n",
    "            \n",
    "            mean_current = np.mean(X_current, axis=0)\n",
    "            mean_other = np.mean(X_other, axis=0)\n",
    "            mean_diff = mean_current - mean_other\n",
    "            \n",
    "            weights = np.dot(precision_current, mean_diff)\n",
    "            \n",
    "            if np.sum(np.abs(weights)) > 0:\n",
    "                weights = np.abs(weights) / np.sum(np.abs(weights))\n",
    "            else:\n",
    "                n_features = X.shape[1]\n",
    "                weights = np.ones(n_features) / n_features\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Inter-class difference weight computation error: {e}\")\n",
    "            return np.ones(X.shape[1]) / X.shape[1]\n",
    "    \n",
    "    def _f_score_weights(self, X, y, current_class):\n",
    "        \"\"\"Compute weights using F-score (ANOVA F-statistics).\"\"\"\n",
    "        try:\n",
    "            from sklearn.feature_selection import f_classif\n",
    "            y_binary = (y == current_class).astype(int)\n",
    "            f_scores, _ = f_classif(X, y_binary)\n",
    "            f_scores = np.nan_to_num(f_scores, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "            \n",
    "            if np.sum(f_scores) > 0:\n",
    "                weights = f_scores / np.sum(f_scores)\n",
    "            else:\n",
    "                weights = np.ones(X.shape[1]) / X.shape[1]\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"F-score weight computation error: {e}\")\n",
    "            return np.ones(X.shape[1]) / X.shape[1]\n",
    "    \n",
    "    def _inverse_covariance_weights(self, X, y=None, current_class=None):\n",
    "        \"\"\"Compute weights using inverse covariance matrix.\"\"\"\n",
    "        try:\n",
    "            if current_class is not None and y is not None:\n",
    "                current_class_mask = (y == current_class)\n",
    "                X_used = X[current_class_mask]\n",
    "                if len(X_used) < 2:\n",
    "                    return np.ones(X.shape[1]) / X.shape[1]\n",
    "            else:\n",
    "                X_used = X\n",
    "            \n",
    "            cov_estimator = LedoitWolf().fit(X_used)\n",
    "            sigma = cov_estimator.covariance_\n",
    "            precision_matrix = np.linalg.pinv(sigma)\n",
    "            \n",
    "            n_features = X_used.shape[1]\n",
    "            unit_vector = np.ones(n_features)\n",
    "            weights = np.dot(precision_matrix, unit_vector)\n",
    "            \n",
    "            if np.sum(np.abs(weights)) > 0:\n",
    "                weights = np.abs(weights) / np.sum(np.abs(weights))\n",
    "            else:\n",
    "                weights = np.ones(n_features) / n_features\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Inverse covariance weight computation error: {e}\")\n",
    "            return np.ones(X.shape[1]) / X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Binary Class Weight Corrector\n",
    "\n",
    "Corrects local weight vectors using global manifold constraints based on the generalized Rayleigh quotient. For each class, treats the problem as binary classification (class c vs. not c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassWeightCorrector(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Weight vector corrector based on binary global manifold constraints.\n",
    "    \n",
    "    For each class c, treats it as a binary classification problem (c vs. not c),\n",
    "    then corrects the local weight vector via generalized Rayleigh quotient manifold constraint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reg_param : float\n",
    "        Regularization parameter to prevent numerical instability\n",
    "    alpha : float\n",
    "        Balance parameter between local weight preservation and global manifold constraint\n",
    "    method : str\n",
    "        Correction method: 'projection' or 'optimization'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reg_param=1e-6, alpha=0.5, method='projection'):\n",
    "        self.reg_param = reg_param\n",
    "        self.alpha = alpha\n",
    "        self.method = method\n",
    "        self.class_stats_ = {}\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit on training data, compute statistics for each class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like of shape (n_samples, n_features)\n",
    "            Training feature matrix\n",
    "        y_train : array-like of shape (n_samples,)\n",
    "            Training labels\n",
    "        \"\"\"\n",
    "        X_train, y_train = check_X_y(X_train, y_train)\n",
    "        self.X_train_ = X_train\n",
    "        self.y_train_ = y_train\n",
    "        self.n_features_ = X_train.shape[1]\n",
    "        self.classes_ = np.unique(y_train)\n",
    "        \n",
    "        self._compute_class_statistics()\n",
    "        return self\n",
    "    \n",
    "    def _compute_class_statistics(self):\n",
    "        \"\"\"Compute statistics for each class.\"\"\"\n",
    "        for cls in self.classes_:\n",
    "            # Get data for class c and not-c\n",
    "            X_c = self.X_train_[self.y_train_ == cls]\n",
    "            X_not_c = self.X_train_[self.y_train_ != cls]\n",
    "            n_c = X_c.shape[0]\n",
    "            n_not_c = X_not_c.shape[0]\n",
    "            \n",
    "            # Compute means\n",
    "            mean_c = np.mean(X_c, axis=0)\n",
    "            mean_not_c = np.mean(X_not_c, axis=0)\n",
    "            global_mean = (n_c * mean_c + n_not_c * mean_not_c) / (n_c + n_not_c)\n",
    "            \n",
    "            # Compute within-class scatter matrices\n",
    "            S_w_c = self._compute_within_class_scatter(X_c, mean_c)\n",
    "            S_w_not_c = self._compute_within_class_scatter(X_not_c, mean_not_c)\n",
    "            S_w = S_w_c + S_w_not_c\n",
    "            \n",
    "            # Compute between-class scatter matrix\n",
    "            S_b = self._compute_between_class_scatter_binary(\n",
    "                mean_c, mean_not_c, n_c, n_not_c, global_mean\n",
    "            )\n",
    "            \n",
    "            # Store statistics\n",
    "            self.class_stats_[cls] = {\n",
    "                'S_w': S_w,\n",
    "                'S_b': S_b,\n",
    "                'mean_c': mean_c,\n",
    "                'mean_not_c': mean_not_c,\n",
    "                'global_mean': global_mean,\n",
    "                'n_c': n_c,\n",
    "                'n_not_c': n_not_c\n",
    "            }\n",
    "    \n",
    "    def _compute_within_class_scatter(self, X, mean):\n",
    "        \"\"\"Compute within-class scatter matrix.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        S_w = np.zeros((n_features, n_features))\n",
    "        for i in range(n_samples):\n",
    "            diff = (X[i] - mean).reshape(-1, 1)\n",
    "            S_w += diff @ diff.T\n",
    "        return S_w\n",
    "    \n",
    "    def _compute_between_class_scatter_binary(self, mean_c, mean_not_c, n_c, n_not_c, global_mean):\n",
    "        \"\"\"Compute between-class scatter matrix for binary classification.\"\"\"\n",
    "        diff_c = (mean_c - global_mean).reshape(-1, 1)\n",
    "        diff_not_c = (mean_not_c - global_mean).reshape(-1, 1)\n",
    "        S_b = n_c * (diff_c @ diff_c.T) + n_not_c * (diff_not_c @ diff_not_c.T)\n",
    "        return S_b\n",
    "    \n",
    "    def correct_weight_vector(self, w_local, class_label):\n",
    "        \"\"\"\n",
    "        Correct local weight vector.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        w_local : array-like of shape (n_features,)\n",
    "            Local weight vector to correct\n",
    "        class_label : int\n",
    "            Class label for the binary classification problem\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        w_corrected : array-like of shape (n_features,)\n",
    "            Corrected weight vector\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'X_train_'):\n",
    "            raise ValueError(\"Model not fitted. Please call fit() first.\")\n",
    "        \n",
    "        if class_label not in self.class_stats_:\n",
    "            raise ValueError(f\"Class {class_label} not in training data\")\n",
    "        \n",
    "        w_local = np.array(w_local).flatten()\n",
    "        if len(w_local) != self.n_features_:\n",
    "            raise ValueError(f\"Weight vector dimension mismatch: expected {self.n_features_}, got {len(w_local)}\")\n",
    "        \n",
    "        # Get statistics for this class\n",
    "        stats = self.class_stats_[class_label]\n",
    "        S_w = stats['S_w']\n",
    "        S_b = stats['S_b']\n",
    "        \n",
    "        # Add regularization\n",
    "        S_w_reg = S_w + self.reg_param * np.eye(self.n_features_)\n",
    "        \n",
    "        if self.method == 'projection':\n",
    "            w_corrected = self._project_to_manifold(w_local, S_b, S_w_reg)\n",
    "        elif self.method == 'optimization':\n",
    "            w_corrected = self._optimize_with_constraints(w_local, S_b, S_w_reg)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown correction method: {self.method}\")\n",
    "        \n",
    "        # Normalize\n",
    "        w_corrected = self._normalize_weight_vector(w_corrected)\n",
    "        return w_corrected\n",
    "    \n",
    "    def _project_to_manifold(self, w_local, S_b, S_w):\n",
    "        \"\"\"Project onto generalized Rayleigh quotient manifold.\"\"\"\n",
    "        try:\n",
    "            # Solve generalized eigenvalue problem: S_b * w = λ * S_w * w\n",
    "            eigenvalues, eigenvectors = eigh(S_b, S_w)\n",
    "            \n",
    "            # Sort by descending eigenvalues\n",
    "            idx = np.argsort(eigenvalues)[::-1]\n",
    "            eigenvectors = eigenvectors[:, idx]\n",
    "            \n",
    "            # Select eigenvector with largest eigenvalue\n",
    "            w_manifold = eigenvectors[:, 0]\n",
    "            \n",
    "            # Compute projection coefficient\n",
    "            coeff = np.dot(w_manifold, w_local) / np.dot(w_manifold, w_manifold)\n",
    "            \n",
    "            # Project onto manifold direction\n",
    "            w_projected = coeff * w_manifold\n",
    "            \n",
    "            # Combine with original vector\n",
    "            w_corrected = self.alpha * w_projected + (1 - self.alpha) * w_local\n",
    "            \n",
    "            return w_corrected\n",
    "        except np.linalg.LinAlgError:\n",
    "            return w_local\n",
    "    \n",
    "    def _optimize_with_constraints(self, w_local, S_b, S_w):\n",
    "        \"\"\"Correct weight vector through optimization.\"\"\"\n",
    "        def objective_function(w, w_local, S_b, S_w, alpha):\n",
    "            \"\"\"Objective: balance local weight preservation and Rayleigh quotient maximization.\"\"\"\n",
    "            w = w.reshape(-1, 1)\n",
    "            w_local = w_local.reshape(-1, 1)\n",
    "            \n",
    "            # Term 1: Difference from local weight\n",
    "            diff_term = np.sum((w - w_local)**2)\n",
    "            \n",
    "            # Term 2: Generalized Rayleigh quotient (maximize)\n",
    "            denominator = w.T @ S_w @ w\n",
    "            if denominator > 1e-10:\n",
    "                rayleigh_quotient = (w.T @ S_b @ w) / denominator\n",
    "            else:\n",
    "                rayleigh_quotient = 0\n",
    "            \n",
    "            # We want to maximize Rayleigh quotient, so negate it in objective\n",
    "            return alpha * diff_term - (1 - alpha) * rayleigh_quotient\n",
    "        \n",
    "        # Use L-BFGS-B optimization\n",
    "        result = minimize(\n",
    "            objective_function,\n",
    "            w_local,\n",
    "            args=(w_local, S_b, S_w, self.alpha),\n",
    "            method='L-BFGS-B',\n",
    "            bounds=[(-1, 1)] * len(w_local),\n",
    "            options={'maxiter': 1000, 'ftol': 1e-8}\n",
    "        )\n",
    "        \n",
    "        return result.x if result.success else w_local\n",
    "    \n",
    "    def _normalize_weight_vector(self, w):\n",
    "        \"\"\"Normalize weight vector.\"\"\"\n",
    "        w_norm = np.linalg.norm(w)\n",
    "        return w / w_norm if w_norm > 1e-10 else w\n",
    "    \n",
    "    def compute_binary_rayleigh_quotient(self, w, class_label):\n",
    "        \"\"\"Compute generalized Rayleigh quotient for a weight vector.\"\"\"\n",
    "        if class_label not in self.class_stats_:\n",
    "            raise ValueError(f\"Class {class_label} not in training data\")\n",
    "        \n",
    "        stats = self.class_stats_[class_label]\n",
    "        S_w = stats['S_w'] + self.reg_param * np.eye(self.n_features_)\n",
    "        S_b = stats['S_b']\n",
    "        \n",
    "        w = w.reshape(-1, 1)\n",
    "        numerator = w.T @ S_b @ w\n",
    "        denominator = w.T @ S_w @ w\n",
    "        \n",
    "        return (numerator / denominator).flatten()[0] if denominator > 1e-10 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Weighted KNN Classifier\n",
    "\n",
    "The final classifier that uses class-specific feature weights to compute distances and performs distance-weighted voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedKNNClassifier:\n",
    "    \"\"\"\n",
    "    Weighted KNN classifier based on local region weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of neighbors to consider\n",
    "    distance_weight_method : str\n",
    "        Method for computing distance weights: 'exponential', 'inverse', 'gaussian'\n",
    "    inv_cov_matrix : array-like, optional\n",
    "        Inverse covariance matrix for Mahalanobis distance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=7, distance_weight_method='exponential', inv_cov_matrix=None):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_row_number = None\n",
    "        self.feature_names = None\n",
    "        self.distance_weight_method = distance_weight_method\n",
    "        self.inv_cov_matrix = inv_cov_matrix\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_row_number, feature_names=None):\n",
    "        \"\"\"Fit the classifier with training data.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_row_number = X_row_number\n",
    "        self.feature_names = feature_names or [f\"Feature_{i}\" for i in range(X_train.shape[1])]\n",
    "    \n",
    "    def calculate_improved_distance_weight(self, distances, method='rank_exponential', top_m=5):\n",
    "        \"\"\"\n",
    "        Improved distance weight calculation that emphasizes nearest neighbors.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        distances : array-like\n",
    "            Sorted distances to neighbors\n",
    "        method : str\n",
    "            Weight method: 'rank_exponential', 'rank_power', 'relative_distance', 'top_m_dominant'\n",
    "        top_m : int\n",
    "            Number of dominant neighbors for 'top_m_dominant' method\n",
    "        \"\"\"\n",
    "        if len(distances) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        distances = np.array(distances)\n",
    "        \n",
    "        if method == 'rank_exponential':\n",
    "            # Rank-based exponential weight: nearest neighbor has highest weight with rapid decay\n",
    "            ranks = np.arange(1, len(distances) + 1)\n",
    "            base = 2.0  # Higher base = faster decay\n",
    "            weights = base ** (-ranks + 1)  # Rank 1: 2^0=1, Rank 2: 2^-1=0.5, etc.\n",
    "            weights = [w * (1/(d+1e-8)) for w, d in zip(weights, distances)]\n",
    "            \n",
    "        elif method == 'rank_power':\n",
    "            # Rank-based power weight\n",
    "            ranks = np.arange(1, len(distances) + 1)\n",
    "            power = 3.0  # Higher power = faster decay\n",
    "            weights = 1.0 / (ranks ** power)\n",
    "            \n",
    "        elif method == 'relative_distance':\n",
    "            # Relative distance weight: based on ratio to nearest distance\n",
    "            min_distance = np.min(distances)\n",
    "            if min_distance == 0:\n",
    "                min_distance = 1e-8\n",
    "            relative_distances = distances / min_distance\n",
    "            weights = np.exp(2.0 * (1 - relative_distances))\n",
    "            \n",
    "        elif method == 'top_m_dominant':\n",
    "            # Only top m neighbors have significant weight\n",
    "            weights = np.zeros(len(distances))\n",
    "            for i in range(min(top_m, len(distances))):\n",
    "                weights[i] = 1.0 if i == 0 else 0.1\n",
    "        else:\n",
    "            # Default: rank-based exponential\n",
    "            ranks = np.arange(1, len(distances) + 1)\n",
    "            weights = 2.0 ** (-ranks + 1)\n",
    "        \n",
    "        # Normalize\n",
    "        weights = np.array(weights)\n",
    "        if np.sum(weights) > 0:\n",
    "            weights = weights / np.sum(weights)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def calculate_distance_weight(self, distances, method='exponential'):\n",
    "        \"\"\"Calculate distance weights: smaller distance = higher weight.\"\"\"\n",
    "        if len(distances) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        distances = np.array(distances) + 1e-8  # Avoid division by zero\n",
    "        \n",
    "        if method == 'inverse':\n",
    "            weights = 1.0 / distances\n",
    "        elif method == 'gaussian':\n",
    "            sigma = np.median(distances)\n",
    "            if sigma < 1e-8:\n",
    "                sigma = 1.0\n",
    "            weights = np.exp(-distances**2 / (2 * sigma**2))\n",
    "        elif method == 'exponential':\n",
    "            weights = np.exp(-distances)\n",
    "        else:\n",
    "            weights = 1.0 / distances\n",
    "        \n",
    "        # Normalize\n",
    "        if np.sum(weights) > 0:\n",
    "            weights = weights / np.sum(weights)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def calculate_weighted_distance(self, target_point, weights):\n",
    "        \"\"\"\n",
    "        Calculate weighted Euclidean distances from target point to all training samples.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of tuples\n",
    "            (row_id, distance, class_label) for each training sample\n",
    "        \"\"\"\n",
    "        weights = weights / np.sum(weights)  # Ensure normalization\n",
    "        \n",
    "        weighted_distances = []\n",
    "        for i in range(len(self.X_train)):\n",
    "            # Feature-weighted Euclidean distance\n",
    "            weighted_diff = weights * (self.X_train[i] - target_point)\n",
    "            distance = np.sqrt(np.sum(weighted_diff ** 2))\n",
    "            weighted_distances.append((self.X_row_number[i], distance, self.y_train[i]))\n",
    "        \n",
    "        return weighted_distances\n",
    "    \n",
    "    def predict_with_distance_weighted_voting(self, target_point, comprehensive_weights, \n",
    "                                               local_region_classes, verbose=False):\n",
    "        \"\"\"\n",
    "        KNN prediction with distance-weighted voting.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        target_point : array-like\n",
    "            Query point to classify\n",
    "        comprehensive_weights : dict\n",
    "            Class-specific feature weight vectors\n",
    "        local_region_classes : list\n",
    "            Classes present in the local region\n",
    "        verbose : bool\n",
    "            Whether to print detailed information\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (predicted_class, max_consistency, knn_results, weighted_votes)\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n=== Distance-Weighted KNN Prediction ===\")\n",
    "            print(f\"Target point: {target_point}\")\n",
    "        \n",
    "        knn_results = {}\n",
    "        weighted_votes = {}\n",
    "        \n",
    "        for cls, weights in comprehensive_weights.items():\n",
    "            if cls not in local_region_classes:\n",
    "                continue\n",
    "            \n",
    "            # Calculate weighted distances\n",
    "            weighted_distances = self.calculate_weighted_distance(target_point, weights)\n",
    "            \n",
    "            # Sort by distance and select k neighbors\n",
    "            weighted_distances.sort(key=lambda x: x[1])\n",
    "            k_neighbors = weighted_distances[:self.k]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nClass {cls} nearest {self.k} neighbors:\")\n",
    "                print(k_neighbors)\n",
    "            \n",
    "            # Extract distances and class information\n",
    "            distances = [item[1] for item in k_neighbors]\n",
    "            neighbor_classes = [item[2] for item in k_neighbors]\n",
    "            \n",
    "            # Compute distance weights\n",
    "            distance_weights = self.calculate_improved_distance_weight(distances)\n",
    "            class_count = sum(1 for _, _, neighbor_class in k_neighbors if neighbor_class == cls)\n",
    "            consistency = class_count / self.k\n",
    "            \n",
    "            # Compute weighted vote (considering distance weight)\n",
    "            weighted_vote = 0.0\n",
    "            for rank, neighbor_class in enumerate(neighbor_classes):\n",
    "                if neighbor_class == cls:\n",
    "                    weighted_vote = consistency * 1/(distances[rank] + 1e-8)\n",
    "                    break\n",
    "            \n",
    "            weighted_votes[cls] = weighted_vote\n",
    "            \n",
    "            # Store detailed results\n",
    "            knn_results[cls] = {\n",
    "                'neighbors': k_neighbors,\n",
    "                'distances': distances,\n",
    "                'distance_weights': distance_weights,\n",
    "                'neighbor_classes': neighbor_classes,\n",
    "                'consistency': weighted_vote,\n",
    "                'weights_used': weights\n",
    "            }\n",
    "        \n",
    "        # Select prediction using weighted votes\n",
    "        if weighted_votes:\n",
    "            predicted_class = max(weighted_votes.items(), key=lambda x: x[1])[0]\n",
    "            max_weighted_consistency = weighted_votes[predicted_class]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nPrediction result:\")\n",
    "                print(f\"  Weighted vote prediction: Class {predicted_class} (consistency: {max_weighted_consistency:.4f})\")\n",
    "            \n",
    "            return predicted_class, max_weighted_consistency, knn_results, weighted_votes\n",
    "        else:\n",
    "            # Fallback to simple KNN\n",
    "            distances = np.linalg.norm(self.X_train - target_point, axis=1)\n",
    "            nearest_indices = np.argsort(distances)[:self.k]\n",
    "            nearest_classes = self.y_train[nearest_indices]\n",
    "            predicted_class = Counter(nearest_classes).most_common(1)[0][0]\n",
    "            return predicted_class, 0.0, {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Classification Pipeline\n",
    "\n",
    "This section demonstrates the complete MCAW-KNN classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcaw_knn_classification(file_path, target_column='quality', test_size=0.2, \n",
    "                                 k_neighbors=7, region_size=20, verbose=False):\n",
    "    \"\"\"\n",
    "    Run the complete MCAW-KNN classification pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the CSV data file\n",
    "    target_column : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    k_neighbors : int\n",
    "        Number of neighbors for KNN\n",
    "    region_size : int\n",
    "        Size of local regions\n",
    "    verbose : bool\n",
    "        Whether to print detailed progress\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Results including accuracy and detailed test information\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========== Step 1: Load and preprocess data ==========\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Step 1: Data Loading and Preprocessing\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Add row number column (actual row number in CSV, starting from 2 since row 1 is header)\n",
    "    df['csv_row_number'] = range(2, len(df) + 2)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Handle categorical columns if present\n",
    "    if 'type' in df.columns:\n",
    "        df['type'] = df['type'].map({'red': 1, 'white': 2, 'Red': 1, 'White': 2}).fillna(0)\n",
    "    \n",
    "    # Separate features, labels, and row indices\n",
    "    row_indices = df['csv_row_number'].values\n",
    "    X = df.drop([target_column, 'csv_row_number'], axis=1).values\n",
    "    y = df[target_column].values\n",
    "    feature_names = df.drop([target_column, 'csv_row_number'], axis=1).columns.tolist()\n",
    "    \n",
    "    print(f\"Data shape: X{X.shape}, y{y.shape}\")\n",
    "    print(f\"Class distribution: {dict(Counter(y))}\")\n",
    "    print(f\"Features: {feature_names}\")\n",
    "    \n",
    "    # ========== Step 2: Train-test split ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 2: Train-Test Split\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, row_indices_train, row_indices_test = train_test_split(\n",
    "        X, y, row_indices,\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    X_train_normalized = minmax_scaler.fit_transform(X_train_scaled)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_normalized = minmax_scaler.transform(X_test_scaled)\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    print(f\"Training class distribution: {dict(Counter(y_train))}\")\n",
    "    \n",
    "    # ========== Step 3: Initialize components ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 3: Initialize MCAW-KNN Components\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Compute inverse covariance matrix for optional Mahalanobis distance\n",
    "    cov_matrix = np.cov(X_train_normalized.T)\n",
    "    try:\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    except np.linalg.LinAlgError:\n",
    "        inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "    \n",
    "    # Initialize weight corrector\n",
    "    corrector = BinaryClassWeightCorrector()\n",
    "    corrector.fit(X_train, y_train)\n",
    "    print(\"✓ Weight corrector initialized\")\n",
    "    \n",
    "    # Initialize weight calculator\n",
    "    weight_calculator = GlobalWeightMatrix(method='lda')\n",
    "    print(\"✓ Weight calculator initialized\")\n",
    "    \n",
    "    # Initialize region builder\n",
    "    region_builder = SmoothLocalRegionBuilder(\n",
    "        k_region=15,\n",
    "        region_size=region_size,\n",
    "        alpha=1.0,\n",
    "        beta=1.0,\n",
    "        confidence_level=0.95,\n",
    "        sigma=0.5\n",
    "    )\n",
    "    region_builder.fit(X_train, y_train, row_indices_train)\n",
    "    print(\"✓ Region builder initialized\")\n",
    "    \n",
    "    # Initialize KNN classifier\n",
    "    knn_classifier = WeightedKNNClassifier(k=k_neighbors, inv_cov_matrix=inv_cov_matrix)\n",
    "    knn_classifier.fit(X_train, y_train, row_indices_train, feature_names)\n",
    "    print(\"✓ KNN classifier initialized\")\n",
    "    \n",
    "    # ========== Step 4: Classification ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 4: Running Classification\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    correct_count = 0\n",
    "    total_count = len(X_test)\n",
    "    results = {'details': {}}\n",
    "    \n",
    "    for target_idx in range(len(X_test)):\n",
    "        target_point = X_test[target_idx]\n",
    "        target_class = y_test[target_idx]\n",
    "        target_row = row_indices_test[target_idx]\n",
    "        \n",
    "        # Build local region\n",
    "        best_region = region_builder.build_local_region(target_point, verbose=False)\n",
    "        \n",
    "        if best_region is not None and len(best_region) > 0:\n",
    "            # Extract local region data\n",
    "            X_region = X_train[best_region]\n",
    "            y_region = y_train[best_region]\n",
    "            \n",
    "            # Compute weights for local region\n",
    "            weight_calculator.fit(X_region, y_region)\n",
    "            comprehensive_weights = weight_calculator.get_class_weight_dict()\n",
    "            \n",
    "            # Apply weight correction\n",
    "            for cls in comprehensive_weights.keys():\n",
    "                class_weight = comprehensive_weights[cls]\n",
    "                corrected_weight = corrector.correct_weight_vector(class_weight, cls)\n",
    "                comprehensive_weights[cls] = corrected_weight\n",
    "            \n",
    "            # Get classes in local region\n",
    "            local_region_classes = list(set(y_region))\n",
    "            \n",
    "            # Make prediction\n",
    "            predicted_class, consistency, knn_results, weighted_votes = \\\n",
    "                knn_classifier.predict_with_distance_weighted_voting(\n",
    "                    target_point, comprehensive_weights, local_region_classes, verbose=verbose\n",
    "                )\n",
    "            \n",
    "            # Check if correct\n",
    "            is_correct = (predicted_class == target_class)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Store result\n",
    "            results['details'][target_idx] = {\n",
    "                'target_row': target_row,\n",
    "                'target_class': target_class,\n",
    "                'predicted_class': predicted_class,\n",
    "                'correct': is_correct,\n",
    "                'consistency': consistency\n",
    "            }\n",
    "            \n",
    "            if verbose or target_idx % 10 == 0:\n",
    "                print(f\"Sample {target_idx+1}/{total_count}: True={target_class}, Pred={predicted_class}, {'✓' if is_correct else '✗'}\")\n",
    "    \n",
    "    # ========== Step 5: Results summary ==========\n",
    "    accuracy = correct_count / total_count\n",
    "    results['accuracy'] = accuracy\n",
    "    results['correct_count'] = correct_count\n",
    "    results['total_count'] = total_count\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Classification Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total test samples: {total_count}\")\n",
    "    print(f\"Correctly classified: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Usage\n",
    "\n",
    "Demonstrate the algorithm on the Wine Quality dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run on Wine Quality dataset\n",
    "# Uncomment and modify the path to run on your data\n",
    "\n",
    "# file_path = 'path/to/your/winequalityN.csv'\n",
    "# results = run_mcaw_knn_classification(\n",
    "#     file_path=file_path,\n",
    "#     target_column='quality',\n",
    "#     test_size=0.2,\n",
    "#     k_neighbors=7,\n",
    "#     region_size=20,\n",
    "#     verbose=False\n",
    "# )\n",
    "# \n",
    "# print(f\"\\nFinal Accuracy: {results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Algorithm Summary\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **SmoothLocalRegionBuilder**: Constructs adaptive local regions using:\n",
    "   - Bayesian smoothing for small sample handling\n",
    "   - Wilson score intervals for conservative estimates\n",
    "   - Gaussian kernel for distance adjustment\n",
    "\n",
    "2. **GlobalWeightMatrix**: Computes class-specific feature weights using:\n",
    "   - LDA-based Rayleigh quotient\n",
    "   - F-score statistics\n",
    "   - Centroid-based ratios\n",
    "\n",
    "3. **BinaryClassWeightCorrector**: Corrects weights using:\n",
    "   - Manifold projection\n",
    "   - Generalized Rayleigh quotient optimization\n",
    "\n",
    "4. **WeightedKNNClassifier**: Final classification using:\n",
    "   - Feature-weighted distances\n",
    "   - Distance-weighted voting\n",
    "\n",
    "### Algorithm Flow:\n",
    "```\n",
    "Input: Query point x\n",
    "  ↓\n",
    "Build local region around x\n",
    "  ↓\n",
    "Compute class-specific feature weights on local region\n",
    "  ↓\n",
    "Apply global manifold correction to weights\n",
    "  ↓\n",
    "Weighted KNN classification with distance voting\n",
    "  ↓\n",
    "Output: Predicted class\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
